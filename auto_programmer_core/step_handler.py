# auto_programmer_core/step_handler.py
import logging
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import hashlib

from .user_interaction import UserInteraction
from .project_state import ProjectState
from .llm_interface import LLMInterface
from .prompt_manager import PromptManager
from .project_builder import ProjectBuilder
from .environment_manager import EnvironmentManager
from .code_runner import CodeRunner

logger = logging.getLogger(__name__)

class StepHandler:
    """
    è´Ÿè´£å¤„ç†å•ä¸ªå¼€å‘æ­¥éª¤çš„æ‰§è¡Œé€»è¾‘ï¼ŒåŒ…å«è‡ªåŠ¨åŒ–æµ‹è¯•ã€é”™è¯¯å¤„ç†å’Œç”¨æˆ·åé¦ˆçš„è¿­ä»£å¾ªç¯ã€‚
    """
    def __init__(self,
                 project_state: ProjectState,
                 llm_interface: LLMInterface,
                 prompt_manager: PromptManager,
                 project_builder: ProjectBuilder,
                 environment_manager: EnvironmentManager,
                 user_interaction: UserInteraction,
                 code_runner: CodeRunner,
                 env_name: str,
                 max_attempts: int,
                 env_manager_type: str):
        self.project_state = project_state
        self.llm_interface = llm_interface
        self.prompt_manager = prompt_manager
        self.project_builder = project_builder
        self.environment_manager = environment_manager
        self.user_interaction = user_interaction
        self.code_runner = code_runner
        self.env_name = env_name
        self.max_attempts = max_attempts
        self.env_manager_type = env_manager_type
        logger.info(f"StepHandler åˆå§‹åŒ–å®Œæˆï¼Œæ¯ä¸ªæ­¥éª¤æœ€å¤šå°è¯• {self.max_attempts} æ¬¡ã€‚")


    def _validate_initial_code_response(self, response: Dict[str, Any]) -> Tuple[bool, str]:
        if not isinstance(response, dict): return False, "LLMå“åº”ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ã€‚"
        if "files" not in response: return False, "JSONå“åº”ä¸­ç¼ºå°‘'files'é”®ã€‚"
        if "usage_guide" not in response: return False, "JSONå“åº”ä¸­ç¼ºå°‘'usage_guide'å¯¹è±¡ã€‚"
        return True, ""

    def _validate_modification_instructions_response(self, response: Dict[str, Any]) -> Tuple[bool, str]:
        if not isinstance(response, dict): return False, "LLMå“åº”ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ã€‚"
        if "modifications" not in response: return False, "JSONå“åº”ä¸­ç¼ºå°‘'modifications'é”®ã€‚"
        if "usage_guide" not in response: return False, "JSONå“åº”ä¸­ç¼ºå°‘'usage_guide'å¯¹è±¡ã€‚"
        return True, ""


    def execute_step(self, step_number: int, step_task: Dict[str, Any], completed_steps: set) -> Tuple[str, Optional[str]]:
        logger.info(f"--- å¼€å§‹æ‰§è¡Œå¼€å‘æ­¥éª¤ {step_number} (å·²å®Œæˆæ­¥éª¤: {completed_steps}) ---")
        dependencies = step_task.get("dependencies", [])
        step_type = step_task.get("step_type", "feature_development")
        if not dependencies and step_type == "feature_development":
            return self._execute_initial_generation_step(step_number, step_task)
        else:
            return self._execute_modification_step(step_number, step_task, completed_steps)

    def _run_unit_tests(self, step_number: int, attempt_number: int, attempt_path: Path, tests_to_run: List[str]) -> Tuple[bool, str]:
        if not tests_to_run:
            logger.info(f"æ­¥éª¤ {step_number} æœªæŒ‡å®šè‡ªåŠ¨åŒ–æµ‹è¯•ï¼Œè·³è¿‡æ­¤ç¯èŠ‚ã€‚")
            return True, ""
        project_workspace = self.project_state.current_workspace
        if not project_workspace:
             return False, "å†…éƒ¨é”™è¯¯ï¼šé¡¹ç›®å·¥ä½œåŒºæœªæ‰¾åˆ°ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•ã€‚"
        stdout, stderr, return_code = self.code_runner.run_tests(
            test_paths=tests_to_run,
            env_name=self.env_name,
            cwd=attempt_path,
            project_workspace=project_workspace
        )
        if return_code == 0:
            print("âœ… è‡ªåŠ¨åŒ–æµ‹è¯•é€šè¿‡ï¼")
            logger.info(f"æ­¥éª¤ {step_number}, å°è¯• {attempt_number} çš„è‡ªåŠ¨åŒ–æµ‹è¯•æˆåŠŸã€‚")
            return True, ""
        else:
            print("âŒ è‡ªåŠ¨åŒ–æµ‹è¯•å¤±è´¥ï¼ŒAIå°†å°è¯•è‡ªåŠ¨ä¿®å¤...")
            logger.warning(f"æ­¥éª¤ {step_number}, å°è¯• {attempt_number} çš„è‡ªåŠ¨åŒ–æµ‹è¯•å¤±è´¥ã€‚è¿”å›ç : {return_code}")
            test_output = (f"--- STDOUT ---\n{stdout}\n\n--- STDERR ---\n{stderr}").strip()
            ai_feedback = (
                "ä½ ç”Ÿæˆçš„ä»£ç æœªèƒ½é€šè¿‡è‡ªåŠ¨åŒ–æµ‹è¯•ã€‚è¯·åˆ†æä¸‹é¢çš„ `pytest` è¾“å‡ºï¼Œå¹¶ä¿®æ­£ä½ çš„ä»£ç ï¼ˆåŒ…æ‹¬åŠŸèƒ½ä»£ç å’Œæµ‹è¯•ä»£ç ï¼‰ã€‚"
                f"é”™è¯¯æ—¥å¿—:\n---\n{test_output}\n---"
            )
            return False, ai_feedback

    def _run_inspection(self, step_number: int, attempt_number: int, completed_steps: set) -> Tuple[bool, str]:
        print("ğŸ•µï¸  ä»£ç å·²ç”Ÿæˆï¼Œæ­£åœ¨æäº¤ç»™â€œä»£ç å®¡æŸ¥å‘˜ & æ¶æ„å®ˆæŠ¤è€…â€è¿›è¡Œå®¡æŸ¥...")
        logger.info(f"å¼€å§‹å¯¹æ­¥éª¤ {step_number} å°è¯• {attempt_number} çš„ä»£ç è¿›è¡Œæ¶æ„æ€§å®¡æŸ¥ã€‚")
        project_definition = self.project_state.get_project_definition()
        step_task = self.project_state.get_task_for_step(step_number)
        step_code_json = self.project_state.load_attempt_code_as_json(step_number, attempt_number)
        architecture_notes = self.project_state.load_architecture_notes()
        if not all([project_definition, step_task, step_code_json]):
            return False, "å†…éƒ¨é”™è¯¯ï¼šæ— æ³•åŠ è½½å®¡æŸ¥æ‰€éœ€çš„æ•°æ®ã€‚"
        previous_steps_summaries = []
        if completed_steps:
            for i in sorted(list(completed_steps)):
                summary = self.project_state.load_step_summary(i)
                if summary:
                    previous_steps_summaries.append({"step_number": i, "summary": summary})
        try:
            prompt = self.prompt_manager.load_and_format_prompt(
                "code_inspector",
                project_definition_json=json.dumps(project_definition, ensure_ascii=False, indent=2),
                previous_steps_summary_json=json.dumps(previous_steps_summaries, ensure_ascii=False, indent=2),
                architecture_notes=architecture_notes,
                step_number=step_number,
                step_description_json=json.dumps(step_task, ensure_ascii=False, indent=2),
                step_code_json=json.dumps(step_code_json, ensure_ascii=False, indent=2)
            )
            inspector_response = self.llm_interface.generate_response(prompt, expect_json=True)
            if not isinstance(inspector_response, dict) or "approved" not in inspector_response:
                return False, "æ£€å¯Ÿå‘˜è¿”å›äº†æ— æ•ˆçš„å“åº”æ ¼å¼ï¼Œè¯·ä¿®æ­£è¾“å‡ºçš„JSONç»“æ„ã€‚"
            is_approved = inspector_response.get("approved", False)
            feedback = inspector_response.get("feedback", "")
            notes_to_add = inspector_response.get("architecture_notes_to_add", "")
            self.project_state.save_inspector_feedback(step_number, attempt_number, feedback if feedback else "å®¡æŸ¥é€šè¿‡")
            if is_approved:
                print("âœ… æ£€å¯Ÿå‘˜å®¡æŸ¥é€šè¿‡ï¼")
                logger.info(f"å®¡æŸ¥é€šè¿‡: æ­¥éª¤ {step_number}, å°è¯• {attempt_number}")
                if notes_to_add:
                    print("ğŸ§  æ¶æ„å®ˆæŠ¤è€…è®°å½•äº†æ–°çš„è®¾è®¡å†³ç­–ã€‚")
                    self.project_state.save_architecture_notes(notes_to_add)
                return True, ""
            else:
                print("âŒ æ£€å¯Ÿå‘˜å‘ç°é—®é¢˜ï¼Œå‡†å¤‡é‡æ–°ç”Ÿæˆä»£ç ...")
                return False, feedback or "ä»£ç å®¡æŸ¥æœªé€šè¿‡ï¼Œä½†æ£€å¯Ÿå‘˜æœªæä¾›å…·ä½“åé¦ˆã€‚"
        except Exception as e:
            logger.error(f"ä»£ç å®¡æŸ¥æµç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            return False, f"ä»£ç å®¡æŸ¥æµç¨‹ä¸­å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}"

    def _execute_initial_generation_step(self, step_number: int, step_task: Dict[str, Any]) -> Tuple[str, Optional[str]]:
        logger.info(f"æ‰§è¡Œåˆå§‹/æ— ä¾èµ–ä»£ç ç”Ÿæˆæµç¨‹ (æ­¥éª¤ {step_number})")
        project_definition = self.project_state.get_project_definition()
        architecture_notes = self.project_state.load_architecture_notes()
        if not all([project_definition, step_task]):
            return 'aborted', None
        attempt_number = 0
        cumulative_feedback: List[str] = [] 
        while attempt_number < self.max_attempts:
            attempt_number += 1
            print(f"\n>>>>> æ­¥éª¤ {step_number} (åˆå§‹ç”Ÿæˆ), å°è¯•ç¬¬ {attempt_number}/{self.max_attempts} æ¬¡ <<<<<")
            logger.info(f"--- Step {step_number} (Initial), Attempt {attempt_number} ---")
            try:
                feedback_section = ""
                if cumulative_feedback:
                    feedback_section = "ä½ ä¹‹å‰çš„å°è¯•å¤±è´¥äº†ã€‚è¯·åˆ†æä¸‹é¢çš„å¤±è´¥å†å²ï¼Œä¿®æ­£ä½ çš„ä»£ç ...\n\n" + "\n---\n".join(cumulative_feedback)
                prompt = self.prompt_manager.load_and_format_prompt(
                    'code_generation_step1',
                    project_definition_json=json.dumps(project_definition, ensure_ascii=False, indent=2),
                    architecture_notes=architecture_notes,
                    step_description_json=json.dumps(step_task, ensure_ascii=False, indent=2),
                    feedback_section=feedback_section
                )
                llm_response = self.llm_interface.generate_response(prompt, expect_json=True)
                is_valid, error_msg = self._validate_initial_code_response(llm_response)
                if not is_valid:
                    cumulative_feedback.append(f"JSONæ ¼å¼é”™è¯¯: {error_msg}")
                    continue
                step_attempt_path = self.project_state.get_step_attempt_path(step_number, attempt_number)
                self.project_state.save_step_code_generation_output(step_number, attempt_number, llm_response)
                build_success = self.project_builder.build_project_structure(
                    step_attempt_path,
                    llm_response.get("files", [])
                )
                if not build_success:
                    cumulative_feedback.append("æ— æ³•æ ¹æ®ä½ è¿”å›çš„JSONæ„å»ºé¡¹ç›®æ–‡ä»¶ç»“æ„ã€‚")
                    continue
                action, result_payload = self._build_and_verify(
                    step_number=step_number,
                    attempt_number=attempt_number,
                    step_attempt_path=step_attempt_path,
                    llm_response=llm_response,
                    completed_steps=set()
                )
                if action == 'success':
                    return 'success', result_payload
                elif action == 'failure' and result_payload:
                    cumulative_feedback.append(result_payload)
                else: 
                    return action, result_payload
            except Exception as e:
                logger.error(f"æ‰§è¡Œåˆå§‹ç”Ÿæˆæ­¥éª¤ {step_number} æœŸé—´å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
                return 'aborted', None
        return 'aborted', None

    def _execute_modification_step(self, step_number: int, step_task: Dict[str, Any], completed_steps: set) -> Tuple[str, Optional[str]]:
        logger.info(f"æ‰§è¡Œä»£ç ä¿®æ”¹æµç¨‹ (æ­¥éª¤ {step_number})")
        project_definition = self.project_state.get_project_definition()
        base_code_path = self.project_state.get_latest_successful_code_path()
        last_step_code_json = self.project_state.load_latest_successful_code_as_json()
        architecture_notes = self.project_state.load_architecture_notes()
        if not all([project_definition, step_task, base_code_path, last_step_code_json is not None]):
            return 'aborted', None
        previous_steps_summaries = []
        if completed_steps:
            for i in sorted(list(completed_steps)):
                summary = self.project_state.load_step_summary(i)
                if summary:
                    previous_steps_summaries.append({"step_number": i, "summary": summary})
        attempt_number = 0
        cumulative_feedback: List[str] = [] 
        while attempt_number < self.max_attempts:
            attempt_number += 1
            print(f"\n>>>>> æ­¥éª¤ {step_number} (ä»£ç ä¿®æ”¹), å°è¯•ç¬¬ {attempt_number}/{self.max_attempts} æ¬¡ <<<<<")
            logger.info(f"--- Step {step_number} (Modification), Attempt {attempt_number} ---")
            try:
                feedback_section = ""
                if cumulative_feedback:
                    feedback_section = "ä½ ä¹‹å‰çš„å°è¯•å¤±è´¥äº†ã€‚è¯·åˆ†æä¸‹é¢çš„å¤±è´¥å†å²...\n\n" + "\n---\n".join(cumulative_feedback)
                last_successful_step_number = max(completed_steps) if completed_steps else 0
                prompt = self.prompt_manager.load_and_format_prompt(
                    'code_modification',
                    project_definition_json=json.dumps(project_definition, ensure_ascii=False, indent=2),
                    architecture_notes=architecture_notes,
                    previous_steps_summary_json=json.dumps(previous_steps_summaries, ensure_ascii=False, indent=2),
                    last_successful_step_number=last_successful_step_number,
                    last_step_code_json=json.dumps(last_step_code_json, ensure_ascii=False, indent=2),
                    step_number=step_number,
                    step_description_json=json.dumps(step_task, ensure_ascii=False, indent=2),
                    feedback_section=feedback_section
                )
                modification_instructions = self.llm_interface.generate_response(prompt, expect_json=True)
                is_valid, error_msg = self._validate_modification_instructions_response(modification_instructions)
                if not is_valid:
                    cumulative_feedback.append(f"JSONæ ¼å¼é”™è¯¯: {error_msg}")
                    continue
                step_attempt_path = self.project_state.get_step_attempt_path(step_number, attempt_number)
                self.project_state.save_step_code_generation_output(step_number, attempt_number, modification_instructions)
                build_success = self.project_builder.apply_modifications(
                    base_path=base_code_path,
                    target_path=step_attempt_path,
                    instructions=modification_instructions.get("modifications", [])
                )
                if not build_success:
                    cumulative_feedback.append("åº”ç”¨ä½ æä¾›çš„ä¿®æ”¹æŒ‡ä»¤å¤±è´¥ã€‚")
                    continue
                action, result_payload = self._build_and_verify(
                    step_number=step_number,
                    attempt_number=attempt_number,
                    step_attempt_path=step_attempt_path,
                    llm_response=modification_instructions,
                    completed_steps=completed_steps
                )
                if action == 'success':
                    return 'success', result_payload
                elif action == 'failure' and result_payload:
                    cumulative_feedback.append(result_payload)
                else:
                    return action, result_payload
            except Exception as e:
                logger.error(f"æ‰§è¡Œä¿®æ”¹æ­¥éª¤ {step_number} æœŸé—´å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
                return 'aborted', None
        return 'aborted', None
    
    def _sanitize_dependency_file(self, dependency_file_path: Path):
        """
        è¯»å–ä¾èµ–æ–‡ä»¶å¹¶ç§»é™¤ä¸åº”å­˜åœ¨çš„Pythonæ ‡å‡†åº“æ¨¡å—ã€‚
        """
        if not dependency_file_path.exists():
            return

        # --- ä¿®æ”¹å¼€å§‹: æ–°å¢'unittest'åˆ°æ ‡å‡†åº“åˆ—è¡¨ ---
        STANDARD_LIBS = {
            "sqlite3", "os", "sys", "json", "re", "datetime", "pathlib", "logging",
            "threading", "multiprocessing", "argparse", "collections", "subprocess",
            "shutil", "glob", "math", "random", "time", "uuid", "configparser",
            "hashlib", "tempfile", "unittest"
        }
        # --- ä¿®æ”¹ç»“æŸ ---

        try:
            lines = dependency_file_path.read_text(encoding='utf-8').splitlines()
            original_count = len(lines)
            sanitized_lines = [
                line for line in lines if line.strip() and not any(
                    line.strip().lower().startswith(lib) for lib in STANDARD_LIBS
                )
            ]
            if len(sanitized_lines) < original_count:
                removed_count = original_count - len(sanitized_lines)
                logger.warning(
                    f"è‡ªåŠ¨ä¿®æ­£ä¾èµ–æ–‡ä»¶: ä» '{dependency_file_path.name}' ä¸­ç§»é™¤äº† {removed_count} ä¸ªä¸åº”å­˜åœ¨çš„æ ‡å‡†åº“ã€‚"
                )
                print(f"âš ï¸  è‡ªåŠ¨ä¿®æ­£ï¼šæ£€æµ‹åˆ°AIå°†æ ‡å‡†åº“ï¼ˆå¦‚ unittestï¼‰é”™è¯¯åœ°å†™å…¥ä¾èµ–æ–‡ä»¶ï¼Œå·²è‡ªåŠ¨ç§»é™¤ã€‚")
                dependency_file_path.write_text("\n".join(sanitized_lines) + "\n", encoding='utf-8')
        except Exception as e:
            logger.error(f"æ¸…ç†ä¾èµ–æ–‡ä»¶ '{dependency_file_path}' æ—¶å‡ºé”™: {e}", exc_info=True)

    def _build_and_verify(self, step_number, attempt_number, step_attempt_path, llm_response, completed_steps) -> Tuple[str, Optional[str]]:
        is_approved, inspector_feedback = self._run_inspection(step_number, attempt_number, completed_steps)
        if not is_approved:
            return 'failure', f"ã€ä»£ç æ£€å¯Ÿå‘˜åé¦ˆã€‘:\n{inspector_feedback}"
        project_workspace = self.project_state.current_workspace
        if not project_workspace: return 'aborted', "é¡¹ç›®å·¥ä½œåŒºæœªæ‰¾åˆ°"
        dependency_file_name = llm_response.get("dependency_file") or "requirements.txt"
        requirements_file_path = step_attempt_path / dependency_file_name
        if requirements_file_path.exists():
            self._sanitize_dependency_file(requirements_file_path)
            if requirements_file_path.stat().st_size > 0:
                install_success, stdout, stderr = self.environment_manager.install_dependencies(
                    project_step_path=step_attempt_path, env_name=self.env_name, project_workspace=project_workspace, dependency_filename=dependency_file_name
                )
                self.project_state.save_dependency_install_log(step_number, attempt_number, stdout, stderr)
                if not install_success:
                    ai_feedback = f"ä¾èµ–å®‰è£…å¤±è´¥ã€‚è¯·ä¿®æ­£ `{dependency_file_name}`ã€‚é”™è¯¯æ—¥å¿—:\n---\n{stderr}\n---"
                    print(f"âŒ ä¾èµ–å®‰è£…å¤±è´¥ï¼ŒAIå°†å°è¯•è‡ªåŠ¨ä¿®å¤...")
                    return 'failure', ai_feedback
        else:
            logger.info(f"æ— ä¾èµ–æ–‡ä»¶ '{requirements_file_path.name}'ï¼Œè·³è¿‡å®‰è£…ã€‚")
        tests_to_run = llm_response.get("tests_to_run", [])
        tests_passed, test_feedback = self._run_unit_tests(step_number, attempt_number, step_attempt_path, tests_to_run)
        if not tests_passed:
            return 'failure', test_feedback
        usage_guide = llm_response.get("usage_guide", {})
        step_task = self.project_state.get_task_for_step(step_number)
        action, feedback = self.user_interaction.prompt_for_manual_execution(
            step_task=step_task,
            usage_guide=usage_guide,
            attempt_path=step_attempt_path,
            env_name=self.env_name,
            env_manager=self.env_manager_type,
            tests_passed=bool(tests_to_run)
        )
        if action == 'success':
            self.project_state.mark_step_as_successful(step_number, step_attempt_path)
            print("\nâœ… æ­¤æ­¥éª¤å·²æ ¹æ®æ‚¨çš„ç¡®è®¤å®Œæˆã€‚æ­£åœ¨ä¸ºä¸‹ä¸€æ­¥ç”Ÿæˆä»£ç æ€»ç»“...")
            if not self._summarize_successful_step(step_number):
                print("âš ï¸ æœªèƒ½ä¸ºè¯¥æˆåŠŸæ­¥éª¤ç”Ÿæˆä»£ç æ€»ç»“ã€‚")
            return 'success', llm_response.get("main_executable")
        if feedback:
            self.project_state.save_user_feedback(step_number, attempt_number, feedback)
            return "failure", f"ã€ç”¨æˆ·åé¦ˆã€‘:\n{feedback}"
        else:
            return action, None

    def _summarize_successful_step(self, step_number: int) -> bool:
        logger.info(f"ä¸ºæˆåŠŸçš„æ­¥éª¤ {step_number} ç”Ÿæˆä»£ç æ€»ç»“...")
        project_def = self.project_state.get_project_definition()
        step_code_json = self.project_state.load_successful_step_code_as_json(step_number)
        if not project_def or not step_code_json:
            return False
        try:
            prompt = self.prompt_manager.load_and_format_prompt(
                "code_summary", 
                project_definition_json=json.dumps(project_def, ensure_ascii=False, indent=2), 
                step_number=step_number, 
                step_code_json=json.dumps(step_code_json, ensure_ascii=False, indent=2)
            )
            summary_response = self.llm_interface.generate_response(prompt, expect_json=True)
            if isinstance(summary_response, dict) and "error" not in summary_response:
                self.project_state.save_step_summary(step_number, summary_response)
                return True
            else:
                return False
        except Exception as e:
            logger.error(f"å‡†å¤‡æˆ–æ‰§è¡Œä»£ç æ€»ç»“æ—¶å¤±è´¥: {e}", exc_info=True)
            return False