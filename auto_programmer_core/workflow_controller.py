# auto_programmer_core/workflow_controller.py
import logging
import json
import shutil 
import sys
from typing import Optional, Tuple
from pathlib import Path
from collections import deque

from rich import print as rprint
from rich.panel import Panel

from .config_manager import ConfigManager
from .prompt_manager import PromptManager
from .llm_interface import LLMInterface
from .user_interaction import UserInteraction
from .project_state import ProjectState
from .project_builder import ProjectBuilder
from .step_handler import StepHandler
from .environment_manager import EnvironmentManager
# --- æ–°å¢å¯¼å…¥ ---
from .code_runner import CodeRunner

logger = logging.getLogger(__name__)

class WorkflowController:
    """
    åè°ƒé¡¹ç›®çš„å„ä¸ªé˜¶æ®µæµç¨‹ã€‚
    """
    def __init__(self,
                 config_manager: ConfigManager,
                 prompt_manager: PromptManager,
                 llm_interface: LLMInterface,
                 user_interaction: UserInteraction,
                 project_state: ProjectState):
        self.config_manager = config_manager
        self.prompt_manager = prompt_manager
        self.llm_interface = llm_interface
        self.user_interaction = user_interaction
        self.project_state = project_state

        self.env_config = config_manager.get_environment_config()
        self.environment_manager = EnvironmentManager(self.env_config)
        self.current_env_name: Optional[str] = None
        
        self.last_main_executable: Optional[str] = None
        
        logger.info("WorkflowController åˆå§‹åŒ–å®Œæˆã€‚")

    # ... (run_clarification_phase, run_initial_refinement_phase ç­‰æ–¹æ³•ä¿æŒä¸å˜) ...
    def setup_environment(self) -> bool:
        """åœ¨å·¥ä½œæµå¼€å§‹æ—¶è®¾ç½®é¡¹ç›®ä¸“ç”¨çš„ç¯å¢ƒ"""
        project_name = self.project_state.get_project_name()
        project_workspace = self.project_state.current_workspace
        if not project_name or not project_workspace:
            logger.error("æ— æ³•è·å–é¡¹ç›®åç§°æˆ–å·¥ä½œåŒºï¼Œæ— æ³•è®¾ç½®ç¯å¢ƒã€‚")
            return False
        
        self.current_env_name = project_name
        
        try:
            return self.environment_manager.setup_project_environment(
                env_name=self.current_env_name,
                project_workspace=project_workspace
            )
        except RuntimeError as e:
            logger.critical(f"ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}", exc_info=True)
            print(f"å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            return False

    def run_clarification_phase(self) -> Tuple[bool, str]:
        """
        æ‰§è¡Œéœ€æ±‚æ¾„æ¸…é˜¶æ®µã€‚
        """
        logger.info("å¼€å§‹æ‰§è¡Œéœ€æ±‚æ¾„æ¸…é˜¶æ®µ...")
        try:
            user_idea = self.user_interaction.get_initial_project_idea()
            if not user_idea:
                logger.warning("ç”¨æˆ·æœªæä¾›é¡¹ç›®æ„æƒ³ï¼Œç»ˆæ­¢æµç¨‹ã€‚")
                print("æœªæä¾›é¡¹ç›®æ„æƒ³ï¼Œæµç¨‹ç»“æŸã€‚")
                return False, ""
            
            self.project_state.save_initial_idea(user_idea)

            prompt = self.prompt_manager.load_and_format_prompt(
                "clarification_questions",
                user_idea=user_idea,
                os_info=sys.platform,
                python_version=self.env_config.get("python_version")
            )

            llm_response = self.llm_interface.generate_response(prompt, expect_json=True)

            if not isinstance(llm_response, dict) or "questions" not in llm_response:
                logger.warning(f"AIæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ¾„æ¸…é—®é¢˜JSONã€‚å“åº”: {llm_response}ã€‚å°†è·³è¿‡æ­¤é˜¶æ®µã€‚")
                return True, "ç”¨æˆ·æœªæä¾›è¡¥å……ä¿¡æ¯ï¼ˆAIæœªèƒ½ç”Ÿæˆé—®é¢˜ï¼‰ã€‚"
            
            user_answers = self.user_interaction.get_clarifying_answers(llm_response)
            return True, user_answers

        except Exception as e:
            logger.exception(f"åœ¨éœ€æ±‚æ¾„æ¸…é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
            print(f"å‘ç”Ÿé”™è¯¯: {e}. è¯·æ£€æŸ¥æ—¥å¿—ã€‚")
            return False, ""

    def run_initial_refinement_phase(self, workspace_path: Path, user_answers: str) -> bool:
        """
        æ‰§è¡Œé¡¹ç›®åˆå§‹æ„æƒ³ç»†åŒ–ä¸ç¯å¢ƒè®¾ç½®é˜¶æ®µã€‚
        """
        logger.info("å¼€å§‹æ‰§è¡Œé¡¹ç›®åˆå§‹æ„æƒ³ç»†åŒ–é˜¶æ®µ...")
        try:
            logger.info(f"é¡¹ç›®å·¥ä½œåŒºå·²åœ¨ {workspace_path} åˆå§‹åŒ–ã€‚")

            if not self.setup_environment():
                logger.critical("é¡¹ç›®ç¯å¢ƒè®¾ç½®å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
                return False
            
            user_idea = (self.project_state._get_workspace_path() / self.project_state.RAW_INPUT_FILENAME).read_text(encoding='utf-8')

            formatted_prompt = self.prompt_manager.load_and_format_prompt(
                "initial_refinement", 
                user_idea=user_idea,
                user_answers=user_answers
            )

            print("\næ­£åœ¨ç»“åˆæ‚¨çš„æ„æƒ³å’Œè¡¥å……ä¿¡æ¯ï¼Œè¿›è¡Œé¡¹ç›®è§„åˆ’ï¼Œè¯·ç¨å€™...")
            logger.info("æ­£åœ¨è¯·æ±‚LLMç»†åŒ–é¡¹ç›®æ„æƒ³...")
            llm_response = self.llm_interface.generate_response(formatted_prompt, expect_json=True)

            if llm_response and isinstance(llm_response, dict) and "error" not in llm_response:
                self.project_state.save_refined_project_description(llm_response)
                self.user_interaction.display_project_description(llm_response)
                logger.info("é¡¹ç›®åˆå§‹æ„æƒ³ç»†åŒ–é˜¶æ®µæˆåŠŸå®Œæˆã€‚")
                return True
            else:
                logger.error(f"LLMæœªèƒ½è¿”å›æœ‰æ•ˆçš„JSONé¡¹ç›®æè¿°ã€‚å“åº”: {llm_response}")
                print("æŠ±æ­‰ï¼Œæ— æ³•ä»AIè·å–æœ‰æ•ˆçš„é¡¹ç›®ç»†åŒ–æè¿°ã€‚")
                return False

        except Exception as e:
            logger.exception(f"åœ¨é¡¹ç›®åˆå§‹æ„æƒ³ç»†åŒ–é˜¶æ®µå‘ç”Ÿæœªé¢„æ–™çš„é”™è¯¯: {e}")
            print(f"å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}. è¯·æ£€æŸ¥æ—¥å¿—ã€‚")
            return False

    def run_project_definition_iteration_phase(self) -> bool:
        """
        æ‰§è¡Œé¡¹ç›®å®šä¹‰çš„è¿­ä»£ç¡®è®¤é˜¶æ®µã€‚
        """
        logger.info("å¼€å§‹é¡¹ç›®å®šä¹‰è¿­ä»£ç¡®è®¤é˜¶æ®µ...")
        current_description = self.project_state.load_refined_project_description()
        if not current_description:
            logger.error("æ— æ³•åŠ è½½åˆå§‹é¡¹ç›®æè¿°ï¼Œæ— æ³•å¼€å§‹è¿­ä»£ã€‚")
            return False

        while True:
            is_satisfied = self.user_interaction.get_confirmation("\næ‚¨å¯¹ä»¥ä¸Šé¡¹ç›®æè¿°æ»¡æ„å—ï¼Ÿ")

            if is_satisfied:
                logger.info("ç”¨æˆ·ç¡®è®¤é¡¹ç›®æè¿°ã€‚")
                self.project_state.save_refined_project_description(current_description)
                print("\nâœ… é¡¹ç›®æè¿°å·²æœ€ç»ˆç¡®è®¤ï¼")
                return True

            feedback = self.user_interaction.get_feedback("è¯·æä¾›æ‚¨çš„ä¿®æ”¹æ„è§ï¼š")
            if not feedback.strip():
                logger.warning("ç”¨æˆ·æœªæä¾›æœ‰æ•ˆåé¦ˆï¼Œè¿­ä»£ä¸­æ­¢ï¼Œæ¥å—å½“å‰ç‰ˆæœ¬ã€‚")
                return True

            prompt = self.prompt_manager.load_and_format_prompt(
                "refinement_iteration",
                previous_description_json=json.dumps(current_description, ensure_ascii=False, indent=4),
                user_feedback=feedback
            )
            
            print("\næ­£åœ¨æ ¹æ®æ‚¨çš„åé¦ˆæ›´æ–°é¡¹ç›®æè¿°ï¼Œè¯·ç¨å€™...")
            llm_response = self.llm_interface.generate_response(prompt, expect_json=True)

            if llm_response and isinstance(llm_response, dict) and "error" not in llm_response:
                current_description = llm_response
                logger.info("LLMè¿”å›äº†ä¿®æ”¹åçš„é¡¹ç›®æè¿°ã€‚")
                print("é¡¹ç›®æè¿°å·²æ›´æ–°ã€‚è¯·å†æ¬¡å®¡é˜…ï¼š")
                self.user_interaction.display_project_description(current_description)
            else:
                logger.error("LLMæœªèƒ½è¿”å›æœ‰æ•ˆçš„ä¿®æ”¹ç‰ˆJSONé¡¹ç›®æè¿°ã€‚")
                print("æŠ±æ­‰ï¼ŒAIæœªèƒ½æ ¹æ®æ‚¨çš„åé¦ˆæœ‰æ•ˆä¿®æ”¹é¡¹ç›®æè¿°ã€‚")

    def run_task_decomposition_phase(self) -> bool:
        """
        æ‰§è¡Œä»»åŠ¡æ‹†åˆ†é˜¶æ®µã€‚
        """
        logger.info("å¼€å§‹ä»»åŠ¡æ‹†åˆ†é˜¶æ®µ...")
        confirmed_description = self.project_state.load_refined_project_description()
        if not confirmed_description:
            logger.error("æœªèƒ½åŠ è½½å·²ç¡®è®¤çš„é¡¹ç›®æè¿°ï¼Œæ— æ³•è¿›è¡Œä»»åŠ¡æ‹†åˆ†ã€‚")
            return False

        prompt = self.prompt_manager.load_and_format_prompt(
            "task_decomposition",
            confirmed_project_description_json=json.dumps(confirmed_description, ensure_ascii=False, indent=4)
        )
        
        print("\né¡¹ç›®æè¿°å·²ç¡®è®¤ï¼Œæ­£åœ¨è¿›è¡Œä»»åŠ¡æ‹†åˆ†ï¼Œè¯·ç¨å€™...")
        logger.info("è¯·æ±‚LLMè¿›è¡Œä»»åŠ¡æ‹†åˆ†...")
        llm_response = self.llm_interface.generate_response(prompt, expect_json=True)

        if llm_response and isinstance(llm_response, dict) and "steps" in llm_response:
            self.project_state.save_task_steps(llm_response)
            self.user_interaction.display_task_steps(llm_response)
            logger.info("ä»»åŠ¡æ‹†åˆ†é˜¶æ®µæˆåŠŸå®Œæˆã€‚")
            return True
        else:
            logger.error(f"LLMæœªèƒ½è¿”å›æœ‰æ•ˆçš„JSONä»»åŠ¡æ­¥éª¤ã€‚å“åº”: {llm_response}")
            print("æŠ±æ­‰ï¼Œæ— æ³•ä»AIè·å–æœ‰æ•ˆçš„ä»»åŠ¡æ‹†åˆ†ç»“æœã€‚")
            return False

    def run_steps_execution_phase(self) -> bool:
        """
        ã€å·²é‡æ„ã€‘æ ¹æ®ä»»åŠ¡ä¾èµ–å›¾ (DAG) æ‰§è¡Œæ‰€æœ‰å·²æ‹†åˆ†çš„å¼€å‘æ­¥éª¤ã€‚
        é‡‡ç”¨æ‹“æ‰‘æ’åºçš„æ‰§è¡Œé€»è¾‘ã€‚
        """
        logger.info("--- å¼€å§‹è¿›å…¥åŸºäºä¾èµ–å›¾çš„ä»£ç ç”Ÿæˆä¸æ‰§è¡Œé˜¶æ®µ ---")
        
        tasks_data = self.project_state.load_task_steps()
        if not tasks_data or not tasks_data.get("steps"):
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•ä»»åŠ¡æ­¥éª¤ï¼Œè·³è¿‡ä»£ç æ‰§è¡Œé˜¶æ®µã€‚")
            return True

        if not self.current_env_name:
            logger.error("å½“å‰é¡¹ç›®ç¯å¢ƒåç§°æœªè®¾ç½®ï¼Œæ— æ³•æ‰§è¡Œæ­¥éª¤ã€‚")
            return False

        execution_config = self.config_manager.get_execution_config()
        project_builder = ProjectBuilder()

        # --- ä¿®æ”¹å¼€å§‹: å®ä¾‹åŒ–å¹¶ä¼ é€’ CodeRunner ---
        code_runner = CodeRunner(
            timeout=execution_config.get("script_timeout_seconds", 300),
            env_config=self.env_config
        )
        step_handler = StepHandler(
            project_state=self.project_state,
            llm_interface=self.llm_interface,
            prompt_manager=self.prompt_manager,
            project_builder=project_builder,
            environment_manager=self.environment_manager,
            user_interaction=self.user_interaction,
            code_runner=code_runner,  # ä¼ é€’å®ä¾‹
            env_name=self.current_env_name,
            max_attempts=execution_config.get("max_step_attempts", 3),
            env_manager_type=self.env_config.get("env_manager", "conda")
        )
        # --- ä¿®æ”¹ç»“æŸ ---

        steps = {step['step_number']: step for step in tasks_data['steps']}
        adj_list = {step_num: [] for step_num in steps}
        in_degree = {step_num: 0 for step_num in steps}

        for step_num, step_data in steps.items():
            dependencies = step_data.get('dependencies', [])
            in_degree[step_num] = len(dependencies)
            for dep_num in dependencies:
                if dep_num in adj_list:
                    adj_list[dep_num].append(step_num)
                else:
                    logger.error(f"ä»»åŠ¡ {step_num} ä¾èµ–ä¸€ä¸ªä¸å­˜åœ¨çš„ä»»åŠ¡ {dep_num}ã€‚æµç¨‹ç»ˆæ­¢ã€‚")
                    print(f"âŒ ä»»åŠ¡å®šä¹‰é”™è¯¯ï¼šä»»åŠ¡ {step_num} çš„ä¾èµ– {dep_num} ä¸å­˜åœ¨ã€‚")
                    return False
        
        queue = deque([step_num for step_num, degree in in_degree.items() if degree == 0])
        completed_steps = set()
        total_steps = len(steps)

        logger.info(f"ä»»åŠ¡ä¾èµ–å›¾è§£æå®Œæˆã€‚æ‹“æ‰‘æ’åºå¼€å§‹ï¼Œåˆå§‹é˜Ÿåˆ—: {list(queue)}")

        while queue:
            step_number = queue.popleft()
            step_task = steps[step_number]
            
            progress_title = f"[æ­¥éª¤ {step_number}/{total_steps}] {step_task.get('step_title', '')}"
            rprint(Panel(f"[bold cyan]{progress_title}[/bold cyan]", border_style="cyan", expand=True))
            
            result, main_executable = step_handler.execute_step(step_number, step_task, completed_steps)
            
            if result == 'success':
                completed_steps.add(step_number)
                if main_executable:
                    self.last_main_executable = main_executable
                
                for dependent_step_num in adj_list[step_number]:
                    in_degree[dependent_step_num] -= 1
                    if in_degree[dependent_step_num] == 0:
                        queue.append(dependent_step_num)
                        logger.info(f"ä»»åŠ¡ {dependent_step_num} çš„æ‰€æœ‰ä¾èµ–å·²å®Œæˆï¼ŒåŠ å…¥å¾…æ‰§è¡Œé˜Ÿåˆ—ã€‚")

            elif result == 'aborted' or result == 'skipped':
                message = "æ‰§è¡Œ" if result == 'aborted' else "è·³è¿‡"
                logger.error(f"æ­¥éª¤ {step_number} çš„{message}å¯¼è‡´æµç¨‹ç»ˆæ­¢ã€‚")
                print(f"âŒ æ­¥éª¤ {step_number} å·²è¢«{message}ï¼Œé¡¹ç›®æµç¨‹ç»ˆæ­¢ã€‚")
                return len(completed_steps) > 0

        if len(completed_steps) == total_steps:
            logger.info("--- æ‰€æœ‰å¼€å‘æ­¥éª¤å‡å·²æ ¹æ®ä¾èµ–å›¾æˆåŠŸæ‰§è¡Œå®Œæ¯• ---")
            print("\nğŸ‰ æ­å–œï¼é¡¹ç›®æ‰€æœ‰å¼€å‘æ­¥éª¤å‡å·²æˆåŠŸå®Œæˆï¼")
            return True
        else:
            logger.error(f"æµç¨‹ç»“æŸï¼Œä½†å¹¶éæ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆã€‚å·²å®Œæˆ: {len(completed_steps)}/{total_steps}ã€‚å¯èƒ½å­˜åœ¨å¾ªç¯ä¾èµ–ã€‚")
            print(f"âŒ é¡¹ç›®æµç¨‹å¼‚å¸¸ç»“æŸã€‚å·²å®Œæˆ {len(completed_steps)}/{total_steps} ä¸ªã€‚è¯·æ£€æŸ¥æ—¥å¿—ï¼ˆå¯èƒ½å­˜åœ¨å¾ªç¯ä¾èµ–ï¼‰ã€‚")
            return False

    def run_finalization_phase(self):
        """
        æ‰§è¡Œé¡¹ç›®å®Œæˆåçš„æœ€ç»ˆæ”¶å°¾é˜¶æ®µã€‚
        """
        logger.info("--- å¼€å§‹è¿›å…¥é¡¹ç›®æœ€ç»ˆæ”¶å°¾é˜¶æ®µ ---")
        rprint(Panel("[bold green]é¡¹ç›®æ”¶å°¾[/bold green]", title="ğŸ", border_style="green", expand=True))

        project_fully_completed = False
        tasks_data = self.project_state.load_task_steps()
        if tasks_data and tasks_data.get("steps"):
            # A bit of a hacky way to check, relies on the last state of completed_steps from the execution phase
            # A better way would be to properly store the final state. For now, this is an approximation.
             final_code_path = self.project_state.get_latest_successful_code_path()
             if final_code_path and any(final_code_path.iterdir()):
                 project_fully_completed = True


        if self.current_env_name and self.project_state.current_workspace:
            env_manager_type = self.env_config.get("env_manager", "conda")
            
            choice, new_name = self.user_interaction.prompt_environment_cleanup_choice(
                self.current_env_name, env_manager_type
            )
            
            if choice == "delete":
                self.environment_manager.delete_environment(
                    self.current_env_name, self.project_state.current_workspace
                )
            elif choice == "rename" and new_name:
                self.environment_manager.rename_environment(
                    self.current_env_name, new_name, self.project_state.current_workspace
                )
            else: 
                print(f"ç¯å¢ƒ '{self.current_env_name}' å·²ä¿ç•™ã€‚")
        
        self.project_state.archive_project()
        
        final_code_path = self.project_state.get_latest_successful_code_path()
        if final_code_path and any(final_code_path.iterdir()):
            env_to_display = self.current_env_name or self.project_state.get_project_name()
            
            guidance_message = (
                f"[bold]é¡¹ç›®æœ€ç»ˆäº§ç‰©æŒ‡å¼•[/bold]\n\n"
                f"âœ… [bold green]æœ€ç»ˆä»£ç å·²ç”Ÿæˆå®Œæ¯•ï¼[/bold green]\n\n"
                f"ğŸ“‚ [bold]ä»£ç è·¯å¾„:[/bold]\n[cyan]{final_code_path.resolve()}[/cyan]\n"
            )
            if self.last_main_executable:
                run_command = ""
                if self.env_config.get("env_manager") == "conda":
                    run_command = f"conda run -n {env_to_display} python {self.last_main_executable}"
                else: 
                    py_exec = ".venv/Scripts/python.exe" if sys.platform == "win32" else "./.venv/bin/python"
                    run_command = f"cd {final_code_path.resolve()} && {py_exec} {self.last_main_executable}"

                guidance_message += f"\nğŸš€ [bold]å»ºè®®è¿è¡Œå‘½ä»¤ (åœ¨æ–°ç»ˆç«¯ä¸­æ‰§è¡Œ):[/bold]\n[cyan]{run_command}[/cyan]\n"
            
            rprint(Panel(guidance_message, title="ğŸ’¡ ä½¿ç”¨æŒ‡å—", border_style="yellow"))
        
        print("\né¡¹ç›®å¼€å‘æµç¨‹å·²å…¨éƒ¨å®Œæˆã€‚æ„Ÿè°¢ä½¿ç”¨ï¼")
        logger.info("--- é¡¹ç›®æœ€ç»ˆæ”¶å°¾é˜¶æ®µå®Œæˆ ---")